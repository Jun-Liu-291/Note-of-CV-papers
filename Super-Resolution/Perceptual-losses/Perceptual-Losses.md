# Perceptual Losses for Real-Time Style Transfer and Super-Resolution

2020-01-04
https://arxiv.org/pdf/1603.08155.pdf

## Abstract
  * Consider image transformation problems, where an input image is transformed into an output image.
  * Recent methods for such problems typically train feed-forward CNN using a per-pixel loss between the output and ground truth images.
  * Parallel work has shown that high-quality images can be generated by defining and optimizing *perceptual* loss functions based on high-level features extracted from pretrained networks.
  
## Introduction
  * Example: denoising, super-resolution and colorization. from computer vision include semantic segmentation and depth estimation.
  * One approach for solving image transformation tasks is to train a feed-forward CNN in a supervised manner, using a per-pixel loss function to measure the difference between output and ground-truth images, such approaches are efficient at test-time, requiring only a forward pass through the trained network.
  * While, this per-pixel loss function do not capture *percepual* differences between output and ground-truth images.
  * Combine the benefits of these two approaches.:
   ** Train feed-forward *transformation networks* for image transformation tasks
   ** Train networks using *perceptual loss functions* that depend on high-level features from a pretrained *loss netowrk*
   ** During training, perceptual losses measure image similarities more robustly than per-pixel losses, and at test-time the transformation networks run in real-time.
